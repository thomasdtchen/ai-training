{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa-IcKgALU3d"
      },
      "source": [
        "# Agentic RAG with Multi-document + Tools Calling + ReAct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5We16Rr2PGG"
      },
      "source": [
        "## Preparation Stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-4np8FN5fFg",
        "outputId": "b7b56d3e-1c94-4453-ac17-540e7455dda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
            "Requirement already satisfied: python-dotenv==1.0.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (1.0.0)\n",
            "Requirement already satisfied: openai==1.23.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (1.23.2)\n",
            "Requirement already satisfied: llama-index==0.10.27 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (0.10.27)\n",
            "Requirement already satisfied: llama-index-core==0.10.27 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (0.10.27)\n",
            "Requirement already satisfied: llama-index-llms-openai==0.1.15 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (0.1.15)\n",
            "Requirement already satisfied: llama-index-embeddings-openai==0.1.7 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (0.1.7)\n",
            "Requirement already satisfied: llama-index-agent-openai==0.2.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (0.2.2)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: httpx==0.27.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (0.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from openai==1.23.2) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from openai==1.23.2) (1.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from openai==1.23.2) (2.10.3)\n",
            "Requirement already satisfied: sniffio in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from openai==1.23.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from openai==1.23.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from openai==1.23.2) (4.12.2)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.13)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.6)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.9.48.post4)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.9)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.22)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index==0.10.27) (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (2024.10.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.16 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (0.1.19)\n",
            "Requirement already satisfied: networkx>=3.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (3.4.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (3.9.1)\n",
            "Requirement already satisfied: numpy in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (2.2.0)\n",
            "Requirement already satisfied: pandas in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (2.2.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (11.0.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (0.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (0.9.0)\n",
            "Requirement already satisfied: wrapt in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-core==0.10.27) (1.17.0)\n",
            "Requirement already satisfied: certifi in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from httpx==0.27.2) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from httpx==0.27.2) (1.0.7)\n",
            "Requirement already satisfied: idna in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from httpx==0.27.2) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from httpcore==1.*->httpx==0.27.2) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.27) (1.18.3)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27) (4.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.27) (0.4.0)\n",
            "Requirement already satisfied: click in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.27) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.27) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.27) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.23.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.23.2) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.10.27) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from requests>=2.31.0->llama-index-core==0.10.27) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.27) (3.1.1)\n",
            "Requirement already satisfied: colorama in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from tqdm>4->openai==1.23.2) (0.4.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.27) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from dataclasses-json->llama-index-core==0.10.27) (3.23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from pandas->llama-index-core==0.10.27) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from pandas->llama-index-core==0.10.27) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from pandas->llama-index-core==0.10.27) (2024.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.27) (2.6)\n",
            "Requirement already satisfied: packaging>=17.0 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.27) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in d:\\git\\core_experiences\\training\\ai\\202412\\ai_workspace\\azure_ai_lab\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.27) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install \\\n",
        "\"python-dotenv==1.0.0\" \\\n",
        "\"openai==1.23.2\" \\\n",
        "\"llama-index==0.10.27\" \\\n",
        "\"llama-index-core==0.10.27\" \\\n",
        "\"llama-index-llms-openai==0.1.15\" \\\n",
        "\"llama-index-embeddings-openai==0.1.7\" \\\n",
        "\"llama-index-agent-openai==0.2.2\" \\\n",
        "\"nest-asyncio==1.6.0\" \\\n",
        "\"httpx==0.27.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6rNJHOe0cRI",
        "outputId": "a1ce064a-45c2-47e2-836a-d3265a0ce283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                                 Version\n",
            "--------------------------------------- ------------\n",
            "aiohappyeyeballs                        2.4.4\n",
            "aiohttp                                 3.11.10\n",
            "aiosignal                               1.3.2\n",
            "annotated-types                         0.7.0\n",
            "anyio                                   4.7.0\n",
            "asttokens                               3.0.0\n",
            "attrs                                   24.3.0\n",
            "azure-ai-vision-imageanalysis           1.0.0b3\n",
            "azure-core                              1.32.0\n",
            "beautifulsoup4                          4.12.3\n",
            "certifi                                 2024.12.14\n",
            "charset-normalizer                      3.4.0\n",
            "click                                   8.1.7\n",
            "colorama                                0.4.6\n",
            "comm                                    0.2.2\n",
            "contourpy                               1.3.1\n",
            "cycler                                  0.12.1\n",
            "dataclasses-json                        0.6.7\n",
            "debugpy                                 1.8.11\n",
            "decorator                               5.1.1\n",
            "Deprecated                              1.2.15\n",
            "dirtyjson                               1.0.8\n",
            "distro                                  1.9.0\n",
            "executing                               2.1.0\n",
            "fonttools                               4.55.3\n",
            "frozenlist                              1.5.0\n",
            "fsspec                                  2024.10.0\n",
            "greenlet                                3.1.1\n",
            "h11                                     0.14.0\n",
            "httpcore                                1.0.7\n",
            "httpx                                   0.27.2\n",
            "idna                                    3.10\n",
            "ipykernel                               6.29.5\n",
            "ipython                                 8.30.0\n",
            "isodate                                 0.7.2\n",
            "jedi                                    0.19.2\n",
            "joblib                                  1.4.2\n",
            "jupyter_client                          8.6.3\n",
            "jupyter_core                            5.7.2\n",
            "kiwisolver                              1.4.7\n",
            "llama-index                             0.10.27\n",
            "llama-index-agent-openai                0.2.2\n",
            "llama-index-cli                         0.1.13\n",
            "llama-index-core                        0.10.27\n",
            "llama-index-embeddings-openai           0.1.7\n",
            "llama-index-indices-managed-llama-cloud 0.1.6\n",
            "llama-index-legacy                      0.9.48.post4\n",
            "llama-index-llms-openai                 0.1.15\n",
            "llama-index-multi-modal-llms-openai     0.1.9\n",
            "llama-index-program-openai              0.1.6\n",
            "llama-index-question-gen-openai         0.1.3\n",
            "llama-index-readers-file                0.1.22\n",
            "llama-index-readers-llama-parse         0.1.6\n",
            "llama-parse                             0.4.0\n",
            "llamaindex-py-client                    0.1.19\n",
            "marshmallow                             3.23.1\n",
            "matplotlib                              3.10.0\n",
            "matplotlib-inline                       0.1.7\n",
            "multidict                               6.1.0\n",
            "mypy-extensions                         1.0.0\n",
            "nest-asyncio                            1.6.0\n",
            "networkx                                3.4.2\n",
            "nltk                                    3.9.1\n",
            "numpy                                   2.2.0\n",
            "openai                                  1.23.2\n",
            "packaging                               24.2\n",
            "pandas                                  2.2.3\n",
            "parso                                   0.8.4\n",
            "pillow                                  11.0.0\n",
            "pip                                     23.1.2\n",
            "platformdirs                            4.3.6\n",
            "prompt_toolkit                          3.0.48\n",
            "propcache                               0.2.1\n",
            "psutil                                  6.1.0\n",
            "pure_eval                               0.2.3\n",
            "pydantic                                2.10.3\n",
            "pydantic_core                           2.27.1\n",
            "Pygments                                2.18.0\n",
            "pyparsing                               3.2.0\n",
            "pypdf                                   4.3.1\n",
            "python-dateutil                         2.9.0.post0\n",
            "python-dotenv                           1.0.0\n",
            "pytz                                    2024.2\n",
            "pywin32                                 308\n",
            "PyYAML                                  6.0.2\n",
            "pyzmq                                   26.2.0\n",
            "regex                                   2024.11.6\n",
            "requests                                2.32.3\n",
            "setuptools                              65.5.0\n",
            "six                                     1.17.0\n",
            "sniffio                                 1.3.1\n",
            "soupsieve                               2.6\n",
            "SQLAlchemy                              2.0.36\n",
            "stack-data                              0.6.3\n",
            "striprtf                                0.0.26\n",
            "tenacity                                8.5.0\n",
            "tiktoken                                0.8.0\n",
            "tornado                                 6.4.2\n",
            "tqdm                                    4.67.1\n",
            "traitlets                               5.14.3\n",
            "typing_extensions                       4.12.2\n",
            "typing-inspect                          0.9.0\n",
            "tzdata                                  2024.2\n",
            "urllib3                                 2.2.3\n",
            "wcwidth                                 0.2.13\n",
            "wrapt                                   1.17.0\n",
            "yarl                                    1.18.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpSGN2ZFLoLc",
        "outputId": "28de246f-2598-44e4-818d-e035252929c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./.env\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./.env\n",
        "OPENAI_API_KEY=sk-GUktlQ4YGz8DdZ1D5f02A83836444b4dAdC7D5F9BcFeFf6d\n",
        "# replace https://api.openai.com/v1 with API transit address\n",
        "OPENAI_BASE_URL=https://pro.aiskt.com/v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TbYO1t3xLxv0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "def get_openai_api_key():\n",
        "    _ = load_dotenv(find_dotenv())\n",
        "    return os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "def get_openai_base_url():\n",
        "    _ = load_dotenv(find_dotenv())\n",
        "    return os.getenv(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# assign corresponding value to api_key before invoking OpenAI(), once setup here, all the following calling from\n",
        "# other frameworks like LlamaIndex and Trulens will inherit and don't need to config for the same\n",
        "import openai\n",
        "openai.api_key = get_openai_api_key()\n",
        "openai.base_url = get_openai_base_url()\n",
        "#OPENAI_API_KEY = get_openai_api_key()\n",
        "#OPENAI_BASE_URL = get_openai_base_url()\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-NhSr1mLyp5"
      },
      "source": [
        "## Router Query Engines of Summary Index and Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z9Tt1URXLyA3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "def get_router_query_engine(\n",
        "    file_path: str,\n",
        "    llm = None,\n",
        "    embed_model = None,\n",
        "):\n",
        "    llm = llm or OpenAI(model=\"gpt-3.5-turbo\")\n",
        "    embed_model = embed_model or OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # try to avoid the following issue: 'NoneType' object is not iterable\n",
        "    '''\n",
        "    /usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py in parser(obj)\n",
        "    --> 101 for embedding in obj.data:\n",
        "        102     data = cast(object, embedding.embedding)\n",
        "    TypeError: 'NoneType' object is not iterable\n",
        "    '''\n",
        "\n",
        "    summary_index = SummaryIndex(\n",
        "        nodes,\n",
        "        #embed_model=embed_model,\n",
        "    )\n",
        "    vector_index = VectorStoreIndex(\n",
        "        nodes,\n",
        "        #embed_model=embed_model,\n",
        "    )\n",
        "\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "        llm=llm,\n",
        "    )\n",
        "    vector_query_engine = vector_index.as_query_engine(\n",
        "        llm=llm,\n",
        "    )\n",
        "\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            \"Useful for summarization questions related to MetaGPT\"\n",
        "        ),\n",
        "    )\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=vector_query_engine,\n",
        "        description=(\n",
        "            \"Useful for retrieving specific context or particular content in detail from the paper of MetaGPT\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    query_engine = RouterQueryEngine(\n",
        "        selector=LLMSingleSelector.from_defaults(),\n",
        "        query_engine_tools=[\n",
        "            summary_tool,\n",
        "            vector_tool,\n",
        "        ],\n",
        "        verbose=True,\n",
        "    )\n",
        "    return query_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TqLv867IMJAh"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UNeoUK8UMJGF"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"Building Agentic RAG with Llamaindex/Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\"],\n",
        ").load_data()\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tWPTQGlRMI56"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to MetaGPT\"\n",
        "    ),\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context or particular content in detail from the paper of MetaGPT\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzUB2aAMtLW",
        "outputId": "a71cb579-02b5-4d8d-cffc-68b59853d2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 0: This choice indicates that the document is useful for summarization questions related to MetaGPT, which aligns with the question asking for the summary of the document..\n",
            "\u001b[0mThe document introduces MetaGPT, a meta-programming framework for multi-agent collaboration in software development. It assigns specific roles to agents like Product Manager, Engineer, Architect, and Project Manager, breaking down complex tasks into subtasks for streamlined workflows. MetaGPT incorporates Standard Operating Procedures (SOPs) to enhance collaboration efficiency and reduce errors, achieving state-of-the-art performance in code generation benchmarks. The framework emphasizes structured communication, workflow adherence, and iterative programming with executable feedback, showcasing superior performance in handling complex software engineering tasks efficiently. Additionally, MetaGPT leverages role specialization, workflow management, and efficient sharing mechanisms to improve problem-solving capabilities in multi-agent systems. The document also discusses the impact of adding executable feedback to MetaGPT, leading to significant performance improvements.\n",
            "35\n",
            "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context or particular content in detail, which aligns with sharing information in detail with other agents..\n",
            "\u001b[0mAgents share information by utilizing a shared message pool where they publish structured messages. This shared message pool allows all agents to exchange messages directly and access messages from other entities transparently. Agents can retrieve required information from the shared pool without the need to inquire about other agents and await their responses, thereby enhancing communication efficiency.\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"What is the summary of the document?\"\n",
        ")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"How do agents share information in detail with other agents?\"\n",
        ")\n",
        "print(str(response))\n",
        "print(len(response.source_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC-6mTWpM_Ma",
        "outputId": "5306b170-5846-4c7b-9e80-7d481e3e1717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results typically involve specific details and context from the paper, making choice 2 the most relevant for retrieving this information..\n",
            "\u001b[0mThe ablation study results show the performance of MetaGPT without feedback compared to other models like AlphaCode, Incoder, CodeGeeX, GPT-4, PaLM Coder, Codex, and Codex + CodeT in terms of Pass rates on the MBPP and HumanEval datasets.\n"
          ]
        }
      ],
      "source": [
        "query_engine = get_router_query_engine(\"Building Agentic RAG with Llamaindex/Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\")\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"Tell me about the ablation study results?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuRnj1D_6UFm"
      },
      "source": [
        "## Building a Multi-document Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9g_1KDvODE-O"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UAQ2z6113NfT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from typing import List, Optional\n",
        "\n",
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "    # vector_query is used to answer questions over a given paper, always leave page_numbers as None unless there is\n",
        "    # a specific page you want to search for\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None,\n",
        "    ) -> str:\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR,\n",
        "            ),\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "\n",
        "        return response\n",
        "\n",
        "    # the tool name is expected a string with maximum length 64 of only alphabet and underscore, etc., here fetching\n",
        "    # the keywords from the long file name\n",
        "    messages = [\n",
        "        ChatMessage(\n",
        "            role=\"system\",\n",
        "            content=\"You're good at summarization. The user prompt is a paper name. Offer a best short version \\\n",
        "            according to the below rules: \\\n",
        "            1. Keep the first 3 words exactly same as the original paper name and remove the word paper \\\n",
        "            2. Reserve the keywords of the paper name especially describing the core concept \\\n",
        "            3. Only alphabet and underscore are allowed, truncate the tail if it exceeds 30 characters\",\n",
        "        ),\n",
        "        ChatMessage(role=\"user\", content=name),\n",
        "    ]\n",
        "    # assure naming conventions of the tool name by applying solid programming codes afterwards\n",
        "    short_name = str(llm.chat(messages).message.content).replace(\" \", \"_\")[:30]\n",
        "    print(\"Tool name: \" + short_name)\n",
        "\n",
        "    # FuntionTool allow users to easily convert a user-defined function into a tool, and when using an agent with\n",
        "    # function calling, the tool selected rely strongly on the name and description of the tools, therefore carefully\n",
        "    # tuning these parameters can result in larges changes in how LLM calls the tools\n",
        "    # ref: https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/tools/\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{short_name}\",\n",
        "        fn=vector_query,\n",
        "        description=(\n",
        "            f\"Useful for retrieving specific context or particular content in detail from the paper of {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{short_name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5FIiohUJ6c6c"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arjvYUOa6c3v",
        "outputId": "e0f2bd03-3547-480a-bfc2-e69b9e5d60a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\n",
            "Tool name: METAGPT_META_PROGRAMMING_MULTI\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\n",
            "Tool name: LONGLO_EFFICIENT_FINE_TUNING\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\n",
            "Tool name: SELF-RAG_LEARNING_RETRIEVE_GEN\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "L7n--SHQ6cxw"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGkrxQaC6cpC",
        "outputId": "54239c86-938b-4547-a04c-a7afe8c9567d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"query\": \"evaluation results\"}\n",
            "assistant: The evaluation dataset used in LongLoRA is called SAFECONV, which aims to enhance research on conversational safety. It provides annotations for unsafe spans in utterances and suggests safe alternative responses. The dataset includes unsafespans, unsafe responses, and safe alternative responses for over 100,000 dialogues from social media platforms. It is compared with other datasets to demonstrate its comprehensiveness in identifying unsafe spans and rewriting unsafe responses to mitigate unsafe behavior in chatbots.\n",
            "\n",
            "Regarding the evaluation results, the models in LongLoRA achieved better perplexity with longer context sizes on the proof-pile and PG19 datasets. Increasing the context window size led to improved perplexity for the Llama2 7B and 13B models. The study also explored the maximum context length that can be fine-tuned on a single 8 × A100 machine, extending the models to 100k, 65536, and 32768 context lengths. While promising results were obtained on these large settings, there was some perplexity degradation on small context sizes for the extended models, attributed to a known limitation of the Position Interpolation method. Additionally, experiments on retrieval in long contexts compared the model with other open Large Language Models (LLMs) on a topic retrieval task, demonstrating the model's performance in retrieving target topics from very long conversations. \n",
            "\n",
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_SELF-RAG_LEARNING_RETRIEVE_GEN with args: {\"input\": \"Self-RAG\"}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"input\": \"LongLoRA\"}\n",
            "assistant: Here are the summaries of Self-RAG and LongLoRA:\n",
            "\n",
            "1. Self-RAG:\n",
            "Self-RAG is a framework designed to enhance the quality and factuality of a large language model through a combination of retrieval and self-reflection. It involves training the model to retrieve relevant passages on-demand, generate text informed by these passages, and self-assess the output using special tokens called reflection tokens. This approach allows the model to tailor its behavior to different task requirements, leading to improved performance on various tasks compared to existing models like ChatGPT and retrieval-augmented Llama2-chat.\n",
            "\n",
            "2. LongLoRA:\n",
            "LongLoRA is an efficient method that extends the context sizes of pre-trained large language models with limited computation cost. It combines improved LoRA with shifted sparse attention to enable context extension while saving on computation costs. LongLoRA demonstrates strong empirical results on various tasks and can extend the context of large language models while retaining their original architectures. It is compatible with existing techniques like Flash-Attention2 and has been shown to be effective and efficient in extending the context window for large language models.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\"\n",
        ")\n",
        "print(str(response), \"\\n\")\n",
        "response = agent.query(\n",
        "    \"Give me a summary of both Self-RAG and LongLoRA\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "g5wLEU6f9e9X"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-LOFTQ_LORA-FINE-TUNING-AWARE_QUANTIZATION_FOR_LARGE_LANGUAGE_MODELS.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-ZIPFORMER_A_FASTER_AND_BETTER_ENCODER_FOR_AUTOMATIC_SPEECH_RECOGNITION.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-VALUES_A_FRAMEWORK_FOR_SYSTEMATIC_VALIDATION_OF_UNCERTAINTY_ESTIMATION_IN_SEMANTIC_SEGMENTATION.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-FINETUNING_TEXT-TO-IMAGE_DIFFUSION_MODELS_FOR_FAIRNESS.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-KNOWLEDGE_CARD_FILLING_LLMS_KNOWLEDGE_GAPS_WITH_PLUG-IN_SPECIALIZED_LANGUAGE_MODELS.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-METRA_SCALABLE_UNSUPERVISED_RL_WITH_METRIC-AWARE_ABSTRACTION.pdf\",\n",
        "    \"Building Agentic RAG with Llamaindex/Paper-META_CONTINUAL_LEARNING_REVISITED_IMPLICITLY_ENHANCING_ONLINE_HESSIAN_APPROXIMATION_VIA_VARIANCE_REDUCTION.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aql6-o_19e5V",
        "outputId": "7f8a5ebd-10e0-43b1-d02b-83a12bcd4f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK.pdf\n",
            "Tool name: METAGPT_Meta_Programming_Multi\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-LONGLORA_EFFICIENT_FINE-TUNING_OF_LONG-CONTEXT_LARGE_LANGUAGE_MODELS.pdf\n",
            "Tool name: LONGLO_EFFICIENT_FINE_TUNING\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-LOFTQ_LORA-FINE-TUNING-AWARE_QUANTIZATION_FOR_LARGE_LANGUAGE_MODELS.pdf\n",
            "Tool name: LOFTQ_LORA_FINE_TUNING_AWARE_Q\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES.pdf\n",
            "Tool name: SWE_BENCH_CAN_LANGUAGE_MODELS_\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-SELF-RAG_LEARNING_TO_RETRIEVE_GENERATE_AND_CRITIQUE_THROUGH_SELF-REFLECTION.pdf\n",
            "Tool name: SELF_RAG_LEARNING_RETRIEVE_GEN\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-ZIPFORMER_A_FASTER_AND_BETTER_ENCODER_FOR_AUTOMATIC_SPEECH_RECOGNITION.pdf\n",
            "Tool name: ZIPFORMER_A_FASTER_BETTER_ENCO\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-VALUES_A_FRAMEWORK_FOR_SYSTEMATIC_VALIDATION_OF_UNCERTAINTY_ESTIMATION_IN_SEMANTIC_SEGMENTATION.pdf\n",
            "Tool name: VALUES_A_FRAMEWORK_FOR_SYSTEMA\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-FINETUNING_TEXT-TO-IMAGE_DIFFUSION_MODELS_FOR_FAIRNESS.pdf\n",
            "Tool name: FINETUNING_TEXT_TO_IMAGE_DIFFU\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-KNOWLEDGE_CARD_FILLING_LLMS_KNOWLEDGE_GAPS_WITH_PLUG-IN_SPECIALIZED_LANGUAGE_MODELS.pdf\n",
            "Tool name: KNOWLEDGE_CARD_FILLING_LLMS_KN\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-METRA_SCALABLE_UNSUPERVISED_RL_WITH_METRIC-AWARE_ABSTRACTION.pdf\n",
            "Tool name: METRA_SCALABLE_UNSUPERVISED_RL\n",
            "Getting tools for paper: Building Agentic RAG with Llamaindex/Paper-META_CONTINUAL_LEARNING_REVISITED_IMPLICITLY_ENHANCING_ONLINE_HESSIAN_APPROXIMATION_VIA_VARIANCE_REDUCTION.pdf\n",
            "Tool name: META_CONTINUAL_LEARNING_REVISI\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "len(all_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjUK5qRT9ezt",
        "outputId": "cc06e93f-3120-4e96-dc2b-ba42b3c3aeae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToolMetadata(description='Useful for summarization questions related to Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES', name='summary_tool_SWE_BENCH_CAN_LANGUAGE_MODELS_', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>)\n",
            "ToolMetadata(description='Useful for summarization questions related to Paper-METAGPT_META_PROGRAMMING_FOR_A_MULTI-AGENT_COLLABORATIVE_FRAMEWORK', name='summary_tool_METAGPT_Meta_Programming_Multi', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>)\n",
            "ToolMetadata(description='Useful for retrieving specific context or particular content in detail from the paper of Paper-SWE-BENCH_CAN_LANGUAGE_MODELS_RESOLVE_REAL-WORLD_GITHUB_ISSUES', name='vector_tool_SWE_BENCH_CAN_LANGUAGE_MODELS_', fn_schema=<class 'pydantic.v1.main.vector_tool_SWE_BENCH_CAN_LANGUAGE_MODELS_'>)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")\n",
        "\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)\n",
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench, just the brief info is enough\"\n",
        ")\n",
        "for tool in tools:\n",
        "  print(tool.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qrp7xcPp-5P9"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are an agent designed to answer queries over a set of given papers. \\\n",
        "    Please always use the tools provided to answer a question. Do not rely on prior knowledge.\",\n",
        "    verbose=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8z7TQ78-5PN",
        "outputId": "5d48fb2c-76d3-410a-fc71-9974a795a9f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench, just the brief info is enough\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_METAGPT_Meta_Programming_Multi with args: {\"input\": \"evaluation dataset\"}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_SWE_BENCH_CAN_LANGUAGE_MODELS_ with args: {\"input\": \"evaluation dataset\"}\n",
            "--------------------------------------------------------------------------------------------------\n",
            "assistant: The evaluation dataset used in MetaGPT consists of HumanEval and MBPP, which include software development tasks with specific requirements and expected outcomes for assessing model performance in code generation. It provides insights into code statistics, document statistics, cost statistics, revision costs, code executability, and encountered errors during coding.\n",
            "\n",
            "On the other hand, the SWE-Bench dataset is a challenging dataset that includes task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. It assesses models on their ability to address real-world GitHub issues using various retrieval mechanisms and prompting styles. The dataset covers instances from different repositories within SWE-Bench, with models facing difficulties in resolving issues. Even the most successful model, Claude 2, can only resolve a small portion of the total issues. The dataset aims to provide a realistic and demanding environment for evaluating language models in software engineering tasks. \n",
            "\n",
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ), remember to analyze the detail approach in each paper first\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LONGLO_EFFICIENT_FINE_TUNING with args: {\"query\": \"detailed approach\"}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_LOFTQ_LORA_FINE_TUNING_AWARE_Q with args: {\"query\": \"detailed approach\"}\n",
            "--------------------------------------------------------------------------------------------------\n",
            "assistant: The detailed approach in the LongLoRA paper involves conducting experiments to test the model multiple times with different random passkey values to assess passkey retrieval accuracy. The model's performance is evaluated at various context lengths, and modifications are made to the max position embeddings to extend the model's capabilities. Ablation studies are conducted on fine-tuning steps and attention patterns to analyze their impact on model performance and efficiency. Various attention patterns such as shift, dilate, block sparse, and stride sparse attention designs are tested to determine their effectiveness during fine-tuning. The goal is to optimize the model's performance for handling longer documents while maintaining efficiency and accuracy.\n",
            "\n",
            "On the other hand, the approach detailed in the LoftQ paper involves a quantization framework called LoRA-Fine-Tuning-aware Quantization (LoftQ). This framework alternates between quantization and low-rank approximation to approximate the original pre-trained weights of Large Language Models (LLMs). By jointly optimizing the initial values of the quantized backbone and low-rank adapters, LoftQ provides a promising initialization for LoRA fine-tuning, which helps alleviate quantization discrepancies and significantly improves generalization in downstream tasks. The method involves alternating optimization steps, where quantization and singular value decomposition (SVD) are applied iteratively to achieve an efficient approximation of the original high-precision weights. LoftQ has minimal computational cost as it can be applied to individual weight matrices and executed in parallel. The approach also allows for the reuse of the initialization obtained by LoftQ for different downstream tasks after applying it once to a pre-trained model.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench, just the brief info is enough\"\n",
        ")\n",
        "print('--------------------------------------------------------------------------------------------------')\n",
        "print(str(response), \"\\n\")\n",
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ), remember to analyze the detail approach in each paper first\"\n",
        ")\n",
        "print('--------------------------------------------------------------------------------------------------')\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7u8yTnQHlPs"
      },
      "source": [
        "## CoT and Agent Reasoning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FmLukrTz-5CM"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "# ref: https://docs.llamaindex.ai/en/stable/examples/agent/react_agent_with_query_engine/\n",
        "march_2022 = SimpleDirectoryReader(\n",
        "    input_files=[\"Building Agentic RAG with Llamaindex/uber_financial_march_2022.pdf\"]\n",
        ").load_data()\n",
        "june_2022 = SimpleDirectoryReader(\n",
        "    input_files=[\"Building Agentic RAG with Llamaindex/uber_financial_june_2022.pdf\"]\n",
        ").load_data()\n",
        "september_2022 = SimpleDirectoryReader(\n",
        "    input_files=[\"Building Agentic RAG with Llamaindex/uber_financial_september_2022.pdf\"]\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "o5kl64F1-4_R"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "66LKNPKhEMN4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "\n",
        "def get_tool(\n",
        "    name,\n",
        "    full_name,\n",
        "    documents = None,\n",
        "):\n",
        "    if not os.path.exists(f\"./{name}\"):\n",
        "        vector_index = VectorStoreIndex.from_documents(documents)\n",
        "        vector_index.storage_context.persist(persist_dir=f\"./{name}\")\n",
        "    else:\n",
        "        vector_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=f\"./{name}\"),\n",
        "        )\n",
        "\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=3,\n",
        "        llm=llm,\n",
        "    )\n",
        "    query_engine_tool = QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=name,\n",
        "            description=(\n",
        "                \"Provides information about Uber quarterly financials ending\"\n",
        "                f\" {full_name}\"\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return query_engine_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bFD5YvcEEMKw"
      },
      "outputs": [],
      "source": [
        "march_tool = get_tool(\"march_2022\", \"March 2022\", documents=march_2022)\n",
        "june_tool = get_tool(\"june_2022\", \"June 2022\", documents=june_2022)\n",
        "september_tool = get_tool(\"september_2022\", \"September 2022\", documents=september_2022)\n",
        "\n",
        "query_engine_tools = [march_tool, june_tool, september_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wo1q6uE2EMEv"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import AgentRunner, ReActAgent\n",
        "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
        "from llama_index.agent.openai import OpenAIAgentWorker\n",
        "\n",
        "#agent_llm = OpenAI(model=\"gpt-4o\")\n",
        "agent_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    query_engine_tools,\n",
        "    llm=agent_llm,\n",
        "    verbose=True,\n",
        "    max_iterations=15,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB2OnAzUFLjv",
        "outputId": "0c56f430-5591-4302-c2f4-ebf0348f7c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mThought: The user is requesting a comprehensive analysis of Uber's R&D expenditures and revenue in 2022, using information from the quarterly financial reports ending in March 2022, June 2022, and September 2022.\n",
            "Action: march_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased by $72 million, or 14%, primarily due to a $63 million increase in stock-based compensation expense. Revenue increased by $4.0 billion, or 136%, primarily driven by increases in Mobility Trip volumes as the business recovers from the impacts of COVID-19, and a $1.5 billion increase in Freight revenue resulting primarily from the acquisition of Transplace in the fourth quarter of 2021.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information from the March 2022 quarterly financial report regarding Uber's R&D expenditures and revenue changes.\n",
            "Action: june_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased by $216 million for the three months ended June 30, 2022, compared to the same period in 2021. For the six months ended June 30, 2022, research and development expenses increased by $288 million compared to the same period in 2021.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information from the June 2022 quarterly financial report regarding Uber's R&D expenditures.\n",
            "Action: september_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses increased significantly, with a notable rise in stock-based compensation and employee headcount costs. Revenue is primarily generated from fees paid by Drivers and Merchants for platform use, with revenue recognized upon completion of a trip.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I have gathered information from the September 2022 quarterly financial report regarding Uber's R&D expenditures and revenue changes.\n",
            "Thought: I can answer without using any more tools. I'll use the user's language to provide a comprehensive report on Uber's R&D expenditures and revenue changes in 2022.\n",
            "Answer: In 2022, Uber experienced significant increases in research and development (R&D) expenses. In March 2022, R&D expenses increased by $72 million, primarily due to a $63 million increase in stock-based compensation expense. By June 2022, R&D expenses had increased by $216 million compared to the same period in 2021, and for the six months ended June 30, 2022, the expenses rose by $288 million compared to the same period in 2021. The September 2022 report highlighted a significant increase in R&D expenses, driven by rises in stock-based compensation and employee headcount costs.\n",
            "\n",
            "On the revenue side, Uber saw substantial growth in 2022. In March 2022, revenue increased by $4.0 billion, primarily due to increases in Mobility Trip volumes and a $1.5 billion increase in Freight revenue from the acquisition of Transplace. The revenue growth was driven by the recovery of the business from the impacts of COVID-19. The September 2022 report mentioned that revenue is primarily generated from fees paid by Drivers and Merchants for platform use, with revenue recognized upon completion of a trip.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "'''\n",
        "response = agent.chat(\n",
        "    \"Analyze the changes in Uber R&D expenditures and revenue in year 2022 and provide a comprehensive report.\"\n",
        ")\n",
        "'''\n",
        "# leverage the power of CoT and ReAct to make GPT 3.5 think like 4\n",
        "response = agent.chat(\n",
        "    \"Analyze the changes in Uber R&D expenditures and revenue in year 2022 and provide a comprehensive report, you need to \\\n",
        "    gather information from the quarterly financials ending Mar. 2022, Jun. 2022 and Sept. 2022 to perform the analysis, \\\n",
        "    don't assume the analysis is completed before gathering similar information for the subsequent quarters if available.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKWLSsrEFLKh",
        "outputId": "05771b6c-80fa-4301-f171-a3fcde179e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mThought: I need to gather information from the quarterly financials ending March 2022, June 2022, and September 2022 to analyze the changes in Uber's R&D expenditures and revenue in 2022.\n",
            "Action: march_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue in March 2022'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenses in March 2022 increased by $72 million compared to the same period in 2021. Revenue in March 2022 increased by $4.0 billion compared to the same period in 2021.\n",
            "\u001b[0mFalse\n"
          ]
        }
      ],
      "source": [
        "# initiate a step-wise execution\n",
        "task = agent.create_task(\n",
        "    \"Analyze the changes in Uber R&D expenditures and revenue in year 2022 and provide a comprehensive report, you need to \\\n",
        "    gather information from the quarterly financials ending Mar. 2022, Jun. 2022 and Sept. 2022 to perform the analysis, \\\n",
        "    don't assume the analysis is completed before gathering similar information for the subsequent quarters if available.\"\n",
        ")\n",
        "\n",
        "step_output = agent.run_step(task.task_id)\n",
        "print(step_output.is_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v8EH1CXjI9l",
        "outputId": "dfbd045a-ab71-4c51-d3be-ec36898df844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Great! Continue with your planned steps, and at last compare the financials data with DiDi for the same period.\n",
            "\u001b[1;3;38;5;200mThought: I need to gather information from the quarterly financials ending June 2022 to continue analyzing the changes in Uber's R&D expenditures and revenue in 2022.\n",
            "Action: june_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue in June 2022'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: R&D expenditures increased by $216 million in June 2022 compared to the same period in 2021, primarily due to increases in stock-based compensation and employee headcount costs. Revenue in June 2022 increased by $4.1 billion, primarily driven by increases in Gross Bookings, with significant contributions from Freight revenue, Mobility Trip volumes, and Delivery revenue.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# let's put human in the loop now\n",
        "step_output = agent.run_step(\n",
        "    task.task_id,\n",
        "    input=\"Great! Continue with your planned steps, and at last compare the financials data with DiDi for the same period.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hax0PDVNjIyV",
        "outputId": "6c098326-688b-4384-bf38-72e739feb772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;3;38;5;200mThought: I need to gather information from the quarterly financials ending September 2022 to complete the analysis of Uber's R&D expenditures and revenue in 2022.\n",
            "Action: september_2022\n",
            "Action Input: {'input': 'R&D expenditures and revenue in September 2022'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Research and development expenditures in September 2022 increased by $267 million compared to the same period in 2021, primarily due to increases in stock-based compensation and employee headcount costs. Revenue in September 2022 increased by $3.5 billion, primarily driven by increases in Gross Bookings, Mobility Trip volumes, Freight Gross Bookings, and changes in business models in the UK.\n",
            "\u001b[0m\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to provide a comprehensive report on the changes in Uber's R&D expenditures and revenue in 2022.\n",
            "Answer: In 2022, Uber experienced significant increases in research and development (R&D) expenditures across the quarters. In March 2022, R&D expenses increased by $72 million compared to the same period in 2021, driven by a rise in stock-based compensation expenses. By June 2022, R&D expenses had increased by $216 million, and in September 2022, there was a further increase of $267 million compared to the same period in 2021.\n",
            "\n",
            "On the revenue side, Uber also saw substantial growth in 2022. Revenue increased by $4.0 billion in March 2022, $4.1 billion in June 2022, and $3.5 billion in September 2022 compared to the respective periods in 2021. The revenue growth was primarily fueled by increases in Gross Bookings, Mobility Trip volumes, Freight revenue, and changes in business models.\n",
            "\n",
            "When comparing Uber's financial data with DiDi for the same period, it would be essential to analyze DiDi's R&D expenditures and revenue to draw meaningful comparisons and insights.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "while not step_output.is_last:\n",
        "     step_output = agent.run_step(task.task_id)\n",
        "step_output = agent.finalize_response(task.task_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXCW6jlGqSla"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "azure_ai_lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
